#!/bin/bash

# Test runner script for UHDM frontend
# Runs test_uhdm_workflow.sh for every test directory and provides comprehensive statistics
# Can also run Yosys tests with --yosys or --all options

# Don't exit on error - we want to run all tests
set +e

# Get script directory
SCRIPT_DIR="$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)"
PROJECT_ROOT="$(cd "$SCRIPT_DIR/.." && pwd)"

# Setup logging
LOG_FILE="$SCRIPT_DIR/test.log"
YOSYS_LOG_FILE="$SCRIPT_DIR/test-yosys.log"

# Create/clear log files
echo "Test run started at $(date)" > "$LOG_FILE"
echo "Test run started at $(date)" > "$YOSYS_LOG_FILE"

# Arrays to track test execution times
declare -A TEST_TIMES
declare -A TEST_START_TIMES

# Default behavior
RUN_LOCAL=true
RUN_YOSYS=false
SPECIFIC_TEST=""

# Parse command line arguments
while [[ $# -gt 0 ]]; do
    case $1 in
        --yosys)
            RUN_LOCAL=false
            RUN_YOSYS=true
            shift
            ;;
        --all)
            RUN_LOCAL=true
            RUN_YOSYS=true
            shift
            ;;
        --help|-h)
            echo "Usage: $0 [OPTIONS] [TEST_PATTERN]"
            echo ""
            echo "Options:"
            echo "  --yosys       Run only Yosys tests from third_party/yosys/tests/"
            echo "  --all         Run both local tests and Yosys tests"
            echo "  --help, -h    Show this help message"
            echo ""
            echo "Arguments:"
            echo "  TEST_PATTERN  Optional pattern to match specific tests"
            echo ""
            echo "Examples:"
            echo "  $0                    # Run all local tests"
            echo "  $0 simple_memory      # Run local test matching 'simple_memory'"
            echo "  $0 --yosys            # Run all Yosys tests"
            echo "  $0 --yosys simple     # Run Yosys tests matching 'simple'"
            echo "  $0 --all              # Run all local and Yosys tests"
            exit 0
            ;;
        *)
            SPECIFIC_TEST="$1"
            shift
            ;;
    esac
done

echo "=== UHDM Frontend Test Runner ==="

# Change to test directory
cd "$SCRIPT_DIR"

# Initialize counters and tracking arrays
TOTAL_TESTS=0
PASSED_TESTS=0
FAILED_TESTS=0
SKIPPED_TESTS=0
CRASHED_TESTS=0
UHDM_ONLY_TESTS=0

# For Yosys tests
YOSYS_TOTAL=0
YOSYS_PASSED=0
YOSYS_FAILED=0
YOSYS_SKIPPED=0
YOSYS_UHDM_ONLY=0
EQUIV_FAILED_TESTS=0

FAILED_TEST_NAMES=()
SKIPPED_TEST_NAMES=()
CRASHED_TEST_NAMES=()
PASSED_TEST_NAMES=()
UHDM_ONLY_TEST_NAMES=()
EQUIV_FAILED_TEST_NAMES=()

# Track unexpected results
UNEXPECTED_FAILURES=()
UNEXPECTED_SUCCESSES=()

# Load failing tests list
FAILING_TESTS=()
if [ -f "failing_tests.txt" ]; then
    while IFS= read -r line; do
        # Skip empty lines and comments
        if [[ -n "$line" && ! "$line" =~ ^[[:space:]]*# ]]; then
            # Trim leading and trailing whitespace and remove inline comments
            trimmed_line=$(echo "$line" | sed 's/#.*//;s/^[[:space:]]*//;s/[[:space:]]*$//')
            if [[ -n "$trimmed_line" ]]; then
                FAILING_TESTS+=("$trimmed_line")
            fi
        fi
    done < "failing_tests.txt"
fi

if [ ${#FAILING_TESTS[@]} -gt 0 ]; then
    echo "Tests marked as failing (will still be run): ${FAILING_TESTS[*]}"
else
    echo "No tests marked as failing - all tests will be run normally"
fi

# Load skipped tests list
SKIPPED_TESTS_LIST=()
if [ -f "skipped_tests.txt" ]; then
    while IFS= read -r line; do
        # Skip empty lines and comments
        if [[ ! "$line" =~ ^[[:space:]]*# ]] && [[ ! "$line" =~ ^[[:space:]]*$ ]]; then
            # Trim leading and trailing whitespace and remove inline comments
            trimmed_line=$(echo "$line" | sed 's/#.*//;s/^[[:space:]]*//;s/[[:space:]]*$//')
            if [[ -n "$trimmed_line" ]]; then
                SKIPPED_TESTS_LIST+=("$trimmed_line")
            fi
        fi
    done < "skipped_tests.txt"
fi

if [ ${#SKIPPED_TESTS_LIST[@]} -gt 0 ]; then
    echo "Tests marked as skipped (will not be run): ${SKIPPED_TESTS_LIST[*]}"
else
    echo "No tests marked as skipped - all tests will be run"
fi
echo

# Only find local test directories if running local tests
if [ "$RUN_LOCAL" = true ]; then
    # Find all test directories (directories containing dut.sv)
    TEST_DIRS=()
    if [ -n "$SPECIFIC_TEST" ] && [ "$RUN_YOSYS" = false ]; then
        # Check if the specific test exists
        if [ -d "$SPECIFIC_TEST" ] && [ -f "$SPECIFIC_TEST/dut.sv" ]; then
            TEST_DIRS+=("$SPECIFIC_TEST")
        else
            echo "Error: Test '$SPECIFIC_TEST' not found or doesn't contain dut.sv"
            exit 1
        fi
    elif [ "$RUN_YOSYS" = false ] || [ -z "$SPECIFIC_TEST" ]; then
        # Find all test directories
        for dir in */; do
            if [ -d "$dir" ] && [ -f "$dir/dut.sv" ]; then
                # Remove trailing slash from directory name
                TEST_NAME="${dir%/}"
                # Skip the run directory (used for Yosys tests)
                if [ "$TEST_NAME" != "run" ]; then
                    TEST_DIRS+=("$TEST_NAME")
                fi
            fi
        done
    fi

    if [ "$RUN_YOSYS" = false ] && [ ${#TEST_DIRS[@]} -eq 0 ]; then
        echo "No test directories found (looking for directories containing dut.sv)"
        exit 1
    fi

    if [ ${#TEST_DIRS[@]} -gt 0 ]; then
        echo "Found ${#TEST_DIRS[@]} local test(s): ${TEST_DIRS[*]}"
        echo
    fi
fi

# Helper function to check if test is in failing list
is_failing_test() {
    local test_name="$1"
    for failing_test in "${FAILING_TESTS[@]}"; do
        if [ "$test_name" = "$failing_test" ]; then
            return 0
        fi
    done
    return 1
}

# Helper function to check if test should be skipped
should_skip_test() {
    local test_name="$1"
    for skipped_test in "${SKIPPED_TESTS_LIST[@]}"; do
        # Support wildcards in skipped test patterns
        if [[ "$test_name" == $skipped_test ]] || [[ "$test_name" == *"$skipped_test"* ]]; then
            return 0
        fi
    done
    return 1
}

# Function to display timing summary
display_timing_summary() {
    if [ ${#TEST_TIMES[@]} -gt 0 ]; then
    echo
    echo "=========================================="
    echo "=== TEST EXECUTION TIME SUMMARY ==="
    echo "=========================================="
    echo
    
    # Sort tests by execution time and display top 5 longest
    echo "Top 5 Longest Running Tests:"
    echo "-----------------------------"
    
    # Create a temporary file for sorting
    TEMP_FILE="/tmp/test_times_$$.txt"
    for test_name in "${!TEST_TIMES[@]}"; do
        printf "%s|%.2f\n" "$test_name" "${TEST_TIMES[$test_name]}"
    done | sort -t'|' -k2 -rn > "$TEMP_FILE"
    
    # Display top 5
    head -5 "$TEMP_FILE" | while IFS='|' read test_name duration; do
        printf "  %-40s %.2f seconds\n" "$test_name" "$duration"
    done
    
    # Calculate total time
    total_time=0
    for duration in "${TEST_TIMES[@]}"; do
        total_time=$(echo "$total_time + $duration" | bc)
    done
    
    echo
    printf "Total test execution time: %.2f seconds\n" $total_time
    
    # Clean up
    rm -f "$TEMP_FILE"
    
    # Log timing summary to file
    {
        echo
        echo "=========================================="
        echo "=== TEST EXECUTION TIME SUMMARY ==="
        echo "=========================================="
        echo
        echo "Top 5 Longest Running Tests:"
        for test_name in "${!TEST_TIMES[@]}"; do
            printf "%s|%.2f\n" "$test_name" "${TEST_TIMES[$test_name]}"
        done | sort -t'|' -k2 -rn | head -5 | while IFS='|' read test_name duration; do
            printf "  %-40s %.2f seconds\n" "$test_name" "$duration"
        done
        echo
        printf "Total test execution time: %.2f seconds\n" $total_time
        echo
        echo "Test run completed at $(date)"
    } >> "$LOG_FILE"
    fi
}

# Helper function to analyze test result
analyze_test_result() {
    local test_dir="$1"
    local exit_code="$2"
    
    # Check for crashes (core dumps, aborts, etc)
    if [ "$exit_code" -eq 134 ] || [ "$exit_code" -eq 139 ] || [ "$exit_code" -eq 6 ]; then
        echo "üí• Test $test_dir CRASHED (exit code: $exit_code)"
        CRASHED_TESTS=$((CRASHED_TESTS + 1))
        CRASHED_TEST_NAMES+=("$test_dir")
        return 1
    fi
    
    # Check if test generated output files
    local uhdm_file="${test_dir}/${test_dir}_from_uhdm.il"
    local verilog_file="${test_dir}/${test_dir}_from_verilog.il"
    local uhdm_synth="${test_dir}/${test_dir}_from_uhdm_synth.v"
    local verilog_synth="${test_dir}/${test_dir}_from_verilog_synth.v"
    
    # Check if UHDM output exists
    if [ ! -f "$uhdm_file" ]; then
        echo "‚ùå Test $test_dir FAILED - UHDM output missing"
        FAILED_TESTS=$((FAILED_TESTS + 1))
        FAILED_TEST_NAMES+=("$test_dir")
        return 1
    fi
    
    # If Verilog output is missing but UHDM succeeded, this might be showcasing UHDM's superior capabilities
    if [ ! -f "$verilog_file" ]; then
        # Check if Verilog frontend failed (common for advanced SystemVerilog)
        if [ -f "${test_dir}/verilog_path.log" ] && grep -q "ERROR" "${test_dir}/verilog_path.log"; then
            echo "‚úÖ Test $test_dir PASSED - UHDM succeeds where Verilog fails!"
            echo "    Demonstrates UHDM's superior SystemVerilog support"
            UHDM_ONLY_TESTS=$((UHDM_ONLY_TESTS + 1))
            UHDM_ONLY_TEST_NAMES+=("$test_dir")
            return 0
        else
            echo "‚ùå Test $test_dir FAILED - Verilog output missing unexpectedly"
            FAILED_TESTS=$((FAILED_TESTS + 1))
            FAILED_TEST_NAMES+=("$test_dir")
            return 1
        fi
    fi
    
    # Check if RTLIL outputs are identical
    local rtlil_identical=false
    if diff -q "$uhdm_file" "$verilog_file" >/dev/null 2>&1; then
        rtlil_identical=true
    fi
    
    # Check if synthesized netlists are identical or have same gate count
    local synth_identical=false
    local gates_match=false
    if [ -f "$uhdm_synth" ] && [ -f "$verilog_synth" ]; then
        # Compare netlists ignoring comments and whitespace
        grep -v "^//" "$uhdm_synth" | grep -v "^$" | sed 's/^[[:space:]]*//' > /tmp/uhdm_synth_clean.tmp
        grep -v "^//" "$verilog_synth" | grep -v "^$" | sed 's/^[[:space:]]*//' > /tmp/verilog_synth_clean.tmp
        if diff -q /tmp/verilog_synth_clean.tmp /tmp/uhdm_synth_clean.tmp >/dev/null 2>&1; then
            synth_identical=true
        fi
        rm -f /tmp/uhdm_synth_clean.tmp /tmp/verilog_synth_clean.tmp
        
        # Run formal equivalence check when both netlists exist
        local equiv_passed=false
        local equiv_failed=false
        if [ -x "./test_equivalence.sh" ]; then
            if ./test_equivalence.sh "$test_dir" >/dev/null 2>&1; then
                echo "    ‚úÖ Formal equivalence check PASSED"
                equiv_passed=true
            else
                echo "    ‚ùå Formal equivalence check FAILED - netlists are not logically equivalent"
                equiv_failed=true
            fi
        fi
    fi
    
    # Report results
    if [ "$equiv_failed" = true ]; then
        echo "‚ùå Test $test_dir FAILED - Formal equivalence check failed"
        EQUIV_FAILED_TESTS=$((EQUIV_FAILED_TESTS + 1))
        EQUIV_FAILED_TEST_NAMES+=("$test_dir")
        return 2  # Return 2 to indicate equivalence failure (different from other failures)
    elif [ "$rtlil_identical" = true ] && [ "$synth_identical" = true ]; then
        echo "‚úÖ Test $test_dir PASSED - Both RTLIL and synthesized netlists are IDENTICAL"
        PASSED_TESTS=$((PASSED_TESTS + 1))
        PASSED_TEST_NAMES+=("$test_dir")
        return 0
    elif [ "$equiv_passed" = true ]; then
        echo "‚úÖ Test $test_dir PASSED - Formal equivalence check confirmed functional equivalence"
        PASSED_TESTS=$((PASSED_TESTS + 1))
        PASSED_TEST_NAMES+=("$test_dir")
        return 0
    elif [ "$rtlil_identical" = true ] && [ "$synth_identical" = false ]; then
        echo "‚ö†Ô∏è  Test $test_dir FAILED - RTLIL identical but synthesized netlists differ"
        EQUIV_FAILED_TESTS=$((EQUIV_FAILED_TESTS + 1))
        EQUIV_FAILED_TEST_NAMES+=("$test_dir")
        return 2
    elif [ "$rtlil_identical" = false ] && [ "$synth_identical" = true ]; then
        echo "‚úÖ Test $test_dir PASSED - RTLIL differs but synthesized netlists are IDENTICAL (functionally equivalent)"
        PASSED_TESTS=$((PASSED_TESTS + 1))
        PASSED_TEST_NAMES+=("$test_dir")
        return 0
    else
        echo "‚ö†Ô∏è  Test $test_dir FUNCTIONAL - Both RTLIL and synthesized netlists differ (no equivalence check performed)"
        # This shouldn't happen with equivalence checking enabled
        FAILED_TESTS=$((FAILED_TESTS + 1))
        FAILED_TEST_NAMES+=("$test_dir")
        
        # Count lines to show size difference
        local uhdm_lines=$(wc -l < "$uhdm_file" 2>/dev/null || echo "0")
        local verilog_lines=$(wc -l < "$verilog_file" 2>/dev/null || echo "0")
        echo "    RTLIL: UHDM=$uhdm_lines lines, Verilog=$verilog_lines lines"
        
        if [ -f "$uhdm_synth" ] && [ -f "$verilog_synth" ]; then
            # Count gates
            local uhdm_gates=$(grep -E '\$_' "$uhdm_synth" | wc -l)
            local verilog_gates=$(grep -E '\$_' "$verilog_synth" | wc -l)
            echo "    Gates: UHDM=$uhdm_gates, Verilog=$verilog_gates"
        fi
        return 1
    fi
}

# Run local tests if requested and found
if [ "$RUN_LOCAL" = true ] && [ ${#TEST_DIRS[@]} -gt 0 ]; then
    for test_dir in "${TEST_DIRS[@]}"; do
    
    # Check if test should be skipped
    if should_skip_test "$test_dir"; then
        echo "=========================================="
        echo "Skipping test: $test_dir (marked in skipped_tests.txt)"
        echo "=========================================="
        SKIPPED_TESTS=$((SKIPPED_TESTS + 1))
        SKIPPED_TEST_NAMES+=("$test_dir")
        echo
        continue
    fi
    
    echo "=========================================="
    echo "Running test: $test_dir"
    echo "=========================================="
    
    TOTAL_TESTS=$((TOTAL_TESTS + 1))
    
    # Check if test is expected to fail
    expected_to_fail=false
    if is_failing_test "$test_dir"; then
        expected_to_fail=true
    fi
    
    # Record start time
    start_time=$(date +%s.%N)
    TEST_START_TIMES["$test_dir"]=$start_time
    
    # Run the test and capture result (don't log verbose output)
    ./test_uhdm_workflow.sh "$test_dir" >/dev/null 2>&1
    exit_code=$?
    
    # Record end time and calculate duration
    end_time=$(date +%s.%N)
    duration=$(echo "$end_time - $start_time" | bc)
    TEST_TIMES["$test_dir"]=$duration
    
    # Display timing info
    printf "Execution time: %.2f seconds\n" $duration
    
    # Analyze the result
    echo -n "Result: "
    test_result=""
    test_passed=false
    analyze_test_result "$test_dir" "$exit_code"
    result_code=$?
    
    # Determine test result type
    if [ $result_code -eq 0 ]; then
        test_passed=true
        test_result="passed"
    elif [ $result_code -eq 2 ]; then
        # Equivalence check failure
        test_result="equiv_failed"
    else
        test_result="failed"
    fi
    
    # Check for unexpected results
    if [ "$expected_to_fail" = true ]; then
        if [ "$test_result" = "passed" ]; then
            echo "    ‚ö†Ô∏è  UNEXPECTED SUCCESS - This test was expected to fail!"
            UNEXPECTED_SUCCESSES+=("$test_dir")
        elif [ "$test_result" = "equiv_failed" ]; then
            echo "    Note: This test was expected to fail (equivalence check failure)"
        else
            echo "    Note: This test was expected to fail"
        fi
    else
        if [ "$test_result" = "failed" ]; then
            echo "    ‚ö†Ô∏è  UNEXPECTED FAILURE - This test was expected to pass!"
            UNEXPECTED_FAILURES+=("$test_dir")
        elif [ "$test_result" = "equiv_failed" ]; then
            echo "    ‚ö†Ô∏è  UNEXPECTED EQUIVALENCE FAILURE - This test should pass formal equivalence!"
            UNEXPECTED_FAILURES+=("$test_dir")
        fi
    fi
    
    echo
    done
fi  # End of local test loop

# Run Yosys tests if requested
if [ "$RUN_YOSYS" = true ]; then
    echo "=========================================="
    echo "=== Running Yosys Tests ==="
    echo "=========================================="
    echo
    
    # Save current directory
    SAVE_DIR=$(pwd)
    
    # Record Yosys tests start time
    yosys_start_time=$(date +%s.%N)
    
    # Run the Yosys test script and capture results
    YOSYS_OUTPUT_FILE="/tmp/yosys_test_output_$$.txt"
    if [ -n "$SPECIFIC_TEST" ] && [ "$RUN_LOCAL" = false ]; then
        "$SCRIPT_DIR/run_yosys_tests.sh" "$SPECIFIC_TEST" 2>&1 | tee "$YOSYS_OUTPUT_FILE"
    else
        "$SCRIPT_DIR/run_yosys_tests.sh" 2>&1 | tee "$YOSYS_OUTPUT_FILE"
    fi
    YOSYS_EXIT_CODE=${PIPESTATUS[0]}
    
    # Record Yosys tests end time
    yosys_end_time=$(date +%s.%N)
    yosys_duration=$(echo "$yosys_end_time - $yosys_start_time" | bc)
    
    # Import individual test times from Yosys tests
    TIMING_FILE="/tmp/yosys_test_times_latest.txt"
    if [ -f "$TIMING_FILE" ]; then
        while IFS='|' read test_name duration; do
            TEST_TIMES["yosys:$test_name"]=$duration
        done < "$TIMING_FILE"
        rm -f "$TIMING_FILE"
    else
        # Fall back to just total time if individual times not available
        TEST_TIMES["Yosys_Tests_Total"]=$yosys_duration
    fi
    
    printf "Yosys tests total execution time: %.2f seconds\n" $yosys_duration
    
    # Return to original directory
    cd "$SAVE_DIR"
    
    # Parse Yosys test results and update global counters
    if [ -f "$YOSYS_OUTPUT_FILE" ]; then
        # Extract and display Yosys test summary
        echo "=== Yosys Test Summary ==="
        
        # Extract the summary section from the output (including statistics and test lists)
        awk '/^=== TEST SUMMARY ===/,/^$/ {print}' "$YOSYS_OUTPUT_FILE" | tail -n +2 || true
        
        # Log Yosys summary to file  
        {
            echo
            echo "=== Yosys Test Summary ==="
            # Get overall statistics
            awk '/^üìä OVERALL STATISTICS:/,/^üéØ Success Rate:/' "$YOSYS_OUTPUT_FILE" | grep -v "^$" || true
            echo ""
            # Get test result lists with their names
            awk '/^‚úÖ PASSED TESTS/,/^(üöÄ|‚ùå|‚ö†Ô∏è|$)/ {if ($0 !~ /^(üöÄ|‚ùå|‚ö†Ô∏è)/ || NR==1) print}' "$YOSYS_OUTPUT_FILE" | grep -v "^$" || true
            awk '/^üöÄ UHDM-ONLY SUCCESS/,/^(‚úÖ|‚ùå|‚ö†Ô∏è|$)/ {if ($0 !~ /^(‚úÖ|‚ùå|‚ö†Ô∏è)/ || NR==1) print}' "$YOSYS_OUTPUT_FILE" | grep -v "^$" || true
            awk '/^‚ùå FAILED TESTS/,/^(‚úÖ|üöÄ|‚ö†Ô∏è|$)/ {if ($0 !~ /^(‚úÖ|üöÄ|‚ö†Ô∏è)/ || NR==1) print}' "$YOSYS_OUTPUT_FILE" | grep -v "^$" || true
            echo ""
            echo "Yosys test duration: $(printf "%.2f" $yosys_duration) seconds"
        } >> "$YOSYS_LOG_FILE"
        
        # Extract test counts from Yosys output to update global counters
        if grep -q "‚úÖ PASSED TESTS" "$YOSYS_OUTPUT_FILE"; then
            YOSYS_PASSED_COUNT=$(grep "‚úÖ PASSED TESTS" "$YOSYS_OUTPUT_FILE" | sed 's/.*(\([0-9]*\)).*/\1/')
            if [ -n "$YOSYS_PASSED_COUNT" ]; then
                PASSED_TESTS=$((PASSED_TESTS + YOSYS_PASSED_COUNT))
                TOTAL_TESTS=$((TOTAL_TESTS + YOSYS_PASSED_COUNT))
            fi
        fi
        
        # Extract UHDM-only success count and names
        if grep -q "üöÄ UHDM-ONLY SUCCESS" "$YOSYS_OUTPUT_FILE"; then
            YOSYS_UHDM_COUNT=$(grep "üöÄ UHDM-ONLY SUCCESS" "$YOSYS_OUTPUT_FILE" | sed 's/.*(\([0-9]*\)).*/\1/')
            if [ -n "$YOSYS_UHDM_COUNT" ] && [ "$YOSYS_UHDM_COUNT" -gt 0 ]; then
                UHDM_ONLY_TESTS=$((UHDM_ONLY_TESTS + YOSYS_UHDM_COUNT))
                TOTAL_TESTS=$((TOTAL_TESTS + YOSYS_UHDM_COUNT))
                
                # Extract UHDM-only test names from the output
                IN_UHDM_SECTION=false
                while IFS= read -r line; do
                    if [[ "$line" =~ ^üöÄ[[:space:]]UHDM-ONLY[[:space:]]SUCCESS ]]; then
                        IN_UHDM_SECTION=true
                    elif [[ "$line" =~ ^(‚úÖ|‚ùå|‚è≠Ô∏è|üí•|$) ]] && [ "$IN_UHDM_SECTION" = true ]; then
                        IN_UHDM_SECTION=false
                    elif [ "$IN_UHDM_SECTION" = true ] && [[ "$line" =~ ^[[:space:]]+-[[:space:]] ]]; then
                        test_name=$(echo "$line" | sed 's/^[[:space:]]*-[[:space:]]*//')
                        UHDM_ONLY_TEST_NAMES+=("yosys:$test_name")
                    fi
                done < "$YOSYS_OUTPUT_FILE"
            fi
        fi
        
        # Extract failed test count
        if grep -q "‚ùå FAILED TESTS" "$YOSYS_OUTPUT_FILE"; then
            YOSYS_FAILED_COUNT=$(grep "‚ùå FAILED TESTS" "$YOSYS_OUTPUT_FILE" | sed 's/.*(\([0-9]*\)).*/\1/')
            if [ -n "$YOSYS_FAILED_COUNT" ]; then
                EQUIV_FAILED_TESTS=$((EQUIV_FAILED_TESTS + YOSYS_FAILED_COUNT))
                TOTAL_TESTS=$((TOTAL_TESTS + YOSYS_FAILED_COUNT))
            fi
        fi
        
        # Clean up
        rm -f "$YOSYS_OUTPUT_FILE"
    fi
    
    echo  # Extra newline after Yosys tests
fi

# Function to print comprehensive summary
print_comprehensive_summary() {
    echo "=========================================="
    echo "=== COMPREHENSIVE TEST SUMMARY ==="
    echo "=========================================="
    echo
    echo "üìà DETAILED BREAKDOWN:"

    if [ $PASSED_TESTS -gt 0 ]; then
        echo
        echo "‚úÖ PERFECT MATCHES ($PASSED_TESTS tests):"
        echo "   These tests produce identical RTLIL output between UHDM and Verilog frontends:"
        for test in "${PASSED_TEST_NAMES[@]}"; do
            echo "   - $test"
        done
    fi

    if [ $UHDM_ONLY_TESTS -gt 0 ]; then
        echo
        echo "üöÄ UHDM-ONLY SUCCESS ($UHDM_ONLY_TESTS tests):"
        echo "   These tests demonstrate UHDM's superior SystemVerilog support:"
        for test in "${UHDM_ONLY_TEST_NAMES[@]}"; do
            echo "   - $test"
        done
    fi

    if [ $CRASHED_TESTS -gt 0 ]; then
        echo
        echo "üí• CRASHED TESTS ($CRASHED_TESTS tests):"
        echo "   These tests crashed during execution and need investigation:"
        for test in "${CRASHED_TEST_NAMES[@]}"; do
            echo "   - $test"
        done
    fi

    if [ $EQUIV_FAILED_TESTS -gt 0 ]; then
        echo
        echo "‚ùå EQUIVALENCE FAILURES ($EQUIV_FAILED_TESTS tests):"
        echo "   These tests generate output but fail formal equivalence checking:"
        for test in "${EQUIV_FAILED_TEST_NAMES[@]}"; do
            echo "   - $test"
        done
    fi

    if [ $FAILED_TESTS -gt 0 ]; then
        echo
        echo "‚ùå TRUE FAILURES ($FAILED_TESTS tests):"
        echo "   These tests failed to generate output files:"
        for test in "${FAILED_TEST_NAMES[@]}"; do
            echo "   - $test"
        done
    fi

    echo
    echo "üîç ANALYSIS:"
    # Recalculate functional tests before displaying
    FUNCTIONAL_TESTS=$((PASSED_TESTS + UHDM_ONLY_TESTS))
    echo "  ‚Ä¢ Tests that work: $FUNCTIONAL_TESTS/$TOTAL_TESTS"
    echo "  ‚Ä¢ Tests that crash: $CRASHED_TESTS/$TOTAL_TESTS"
    echo "  ‚Ä¢ Tests that fail equivalence: $EQUIV_FAILED_TESTS/$TOTAL_TESTS"
    echo "  ‚Ä¢ Tests that fail to generate output: $FAILED_TESTS/$TOTAL_TESTS"
}

# Final summary - show when any tests were run
if [ "$TOTAL_TESTS" -gt 0 ]; then
    # Print to console
    print_comprehensive_summary

# Check for any unexpected results
if [ ${#UNEXPECTED_FAILURES[@]} -gt 0 ]; then
    echo
    echo "‚ùå UNEXPECTED FAILURES (${#UNEXPECTED_FAILURES[@]} tests):"
    echo "   These tests were expected to pass but failed:"
    for test in "${UNEXPECTED_FAILURES[@]}"; do
        echo "   - $test"
    done
fi


if [ ${#UNEXPECTED_SUCCESSES[@]} -gt 0 ]; then
    echo
    echo "‚ùå UNEXPECTED SUCCESSES (${#UNEXPECTED_SUCCESSES[@]} tests):"
    echo "   These tests were expected to fail but passed:"
    for test in "${UNEXPECTED_SUCCESSES[@]}"; do
        echo "   - $test"
    done
    echo
    echo "   Please remove these from failing_tests.txt"
fi

echo
echo "üìä OVERALL STATISTICS:"
echo "  Total tests run: $TOTAL_TESTS"
echo "  ‚úÖ Passing tests: $PASSED_TESTS"
echo "  üöÄ UHDM-only success: $UHDM_ONLY_TESTS"
echo "  ‚ùå Equivalence failures: $EQUIV_FAILED_TESTS"
echo "  ‚ùå True failures: $FAILED_TESTS"
echo "  üí• Crashes: $CRASHED_TESTS"
echo

# Log the comprehensive summary to file
{
    print_comprehensive_summary
    echo
    echo "üìä OVERALL STATISTICS:"
    echo "  Total tests run: $TOTAL_TESTS"
    echo "  ‚úÖ Passing tests: $PASSED_TESTS"
    echo "  üöÄ UHDM-only success: $UHDM_ONLY_TESTS"
    echo "  ‚ùå Equivalence failures: $EQUIV_FAILED_TESTS"
    echo "  ‚ùå True failures: $FAILED_TESTS"
    echo "  üí• Crashes: $CRASHED_TESTS"
    echo
} >> "$LOG_FILE"

# Calculate success rate
FUNCTIONAL_TESTS=$((PASSED_TESTS + UHDM_ONLY_TESTS))
if [ $TOTAL_TESTS -gt 0 ]; then
    SUCCESS_RATE=$((FUNCTIONAL_TESTS * 100 / TOTAL_TESTS))
    echo "üéØ Success Rate: $SUCCESS_RATE% ($FUNCTIONAL_TESTS/$TOTAL_TESTS tests functional)"
    echo "üéØ Success Rate: $SUCCESS_RATE% ($FUNCTIONAL_TESTS/$TOTAL_TESTS tests functional)" >> "$LOG_FILE"
fi

# Determine exit status
echo
if [ ${#UNEXPECTED_FAILURES[@]} -eq 0 ] && [ ${#UNEXPECTED_SUCCESSES[@]} -eq 0 ]; then
    # Count expected failures
    EXPECTED_FAILS=0
    for test in "${FAILED_TEST_NAMES[@]}"; do
        if is_failing_test "$test"; then
            EXPECTED_FAILS=$((EXPECTED_FAILS + 1))
        fi
    done
    for test in "${EQUIV_FAILED_TEST_NAMES[@]}"; do
        if is_failing_test "$test"; then
            EXPECTED_FAILS=$((EXPECTED_FAILS + 1))
        fi
    done
    for test in "${CRASHED_TEST_NAMES[@]}"; do
        if is_failing_test "$test"; then
            EXPECTED_FAILS=$((EXPECTED_FAILS + 1))
        fi
    done
    
    if [ $CRASHED_TESTS -eq 0 ] && [ $FAILED_TESTS -eq 0 ] && [ $EQUIV_FAILED_TESTS -eq 0 ]; then
        echo "üéâ EXCELLENT! All tests are functional! üéâ"
        display_timing_summary
        exit 0
    else
        # Check if all failures are expected
        TOTAL_FAILURES=$((CRASHED_TESTS + FAILED_TESTS + EQUIV_FAILED_TESTS))
        if [ $EXPECTED_FAILS -eq $TOTAL_FAILURES ] && [ $EXPECTED_FAILS -gt 0 ]; then
            echo "‚úÖ ALL RESULTS AS EXPECTED - Test suite passes with known issues"
            echo
            echo "All failing tests are documented in failing_tests.txt:"
            echo "  ‚Ä¢ Expected failures: $EXPECTED_FAILS"
            echo "  ‚Ä¢ Functional tests: $FUNCTIONAL_TESTS/$TOTAL_TESTS"
            echo
            echo "The test suite passes because all results match expectations."
            display_timing_summary
            exit 0
        else
            echo "‚ùå TEST SUITE FAILED - There are failures!"
            echo
            echo "Test results:"
            echo "  ‚Ä¢ Crashed tests: $CRASHED_TESTS"
            echo "  ‚Ä¢ Equivalence failures: $EQUIV_FAILED_TESTS"
            echo "  ‚Ä¢ Failed tests: $FAILED_TESTS"
            echo "  ‚Ä¢ Functional tests: $FUNCTIONAL_TESTS/$TOTAL_TESTS"
            echo
            UNEXPECTED_COUNT=$((TOTAL_FAILURES - EXPECTED_FAILS))
            if [ $UNEXPECTED_COUNT -gt 0 ]; then
                echo "‚ùå Found $UNEXPECTED_COUNT unexpected failures not in failing_tests.txt"
            fi
            echo
            echo "Please investigate failures or update failing_tests.txt"
            display_timing_summary
            exit 1
        fi
    fi
else
    echo "‚ùå TEST SUITE FAILED - Unexpected results detected!"
    echo
    if [ ${#UNEXPECTED_FAILURES[@]} -gt 0 ]; then
        echo "‚Ä¢ ${#UNEXPECTED_FAILURES[@]} tests failed unexpectedly"
    fi
    if [ ${#UNEXPECTED_SUCCESSES[@]} -gt 0 ]; then
        echo "‚Ä¢ ${#UNEXPECTED_SUCCESSES[@]} tests passed unexpectedly"
    fi
    echo
    echo "Please investigate unexpected results or update failing_tests.txt"
    display_timing_summary
    exit 1
fi

fi  # End of local test summary